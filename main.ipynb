{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting date time, contact-name, and message from the chat logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time(s):\n",
    "    pattern = '^([0-9]+(\\/)([0-9]+)(\\/)[0-9]+, ([0-9]+):([0-9]+)\\s(PM|AM|am|pm) - )'\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True \n",
    "    return False\n",
    "\n",
    "def contact(s):\n",
    "    s=s.split(\":\")\n",
    "    if len(s) == 2:\n",
    "        return True \n",
    "    return False\n",
    "\n",
    "def getmsg(line):\n",
    "    splitline = line.split(' - ')\n",
    "    date, time = splitline[0].split(', ')\n",
    "    msg = \" \".join(splitline[1:])\n",
    "   \n",
    "    if contact(msg):\n",
    "        split_msg = msg.split(': ')\n",
    "        author = split_msg[0]\n",
    "        msg = \" \".join(split_msg[1:])\n",
    "    else:\n",
    "        author = None\n",
    "    return date, time, author, msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "conversation = 'chats/WhatsApp Chat with Vishal Sir.txt'\n",
    "with open(conversation, encoding=\"utf-8\") as fp:\n",
    "    fp.readline()\n",
    "    msgBuffer = []\n",
    "    date, time, author=None, None, None\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip()\n",
    "\n",
    "        if date_time(line):\n",
    "            if len(msgBuffer) > 0 :\n",
    "                data.append([date, time, author, \" \".join(msgBuffer)])\n",
    "            msgBuffer.clear()\n",
    "            date, time, author, msg = getmsg(line)\n",
    "            msgBuffer.append(msg)\n",
    "        else:\n",
    "            msgBuffer.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the extracted data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=[\"Date\", \"Time\", \"Contact\", \"Message\"])\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
    "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the deleted messages and media ommitio logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Message\"] = data['Message'][data[\"Message\"] != \"<Media omitted>\"]\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "string_to_match = \" deleted this message\"\n",
    "data = data[~data[\"Message\"].str.contains(string_to_match, case=False)]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the messages from punctuations and stopwords and tokenized the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_message'] = data[\"Message\"].apply(lambda x: x.lower().translate(str.maketrans(' ', \" \", string.punctuation)))\n",
    "\n",
    "data['Tokenized_words'] = data[\"Cleaned_message\"].apply(lambda y: [word for word in word_tokenize(y) if word not in STOPWORDS and word not in stop_words] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the lemmatization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "data[\"Lemmatized\"] = [[lemmatizer.lemmatize(token) for token in token_list] for token_list in data[\"Tokenized_words\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments of each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "\n",
    "data['Positive'] = [sentiments.polarity_scores(i)['pos'] for i in data[\"Message\"]]\n",
    "data['Negative'] = [sentiments.polarity_scores(i)['neg'] for i in data[\"Message\"]]\n",
    "data['Neutral'] = [sentiments.polarity_scores(i)['neu'] for i in data[\"Message\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values of the sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Positive\"] = data[\"Positive\"].apply(lambda x: np.ceil(x) if x - np.floor(x) >= 0.5 else np.floor(x))\n",
    "data[\"Negative\"] = data[\"Negative\"].apply(lambda x: np.ceil(x) if x - np.floor(x) >= 0.5 else np.floor(x))\n",
    "data[\"Neutral\"] = data[\"Neutral\"].apply(lambda x: np.ceil(x) if x - np.floor(x) >= 0.5 else np.floor(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over all sentiment of the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sum(data['Positive'])\n",
    "neg = sum(data['Negative'])\n",
    "neu = sum(data['Neutral'])\n",
    "\n",
    "def score(a,b,c):\n",
    "    if a>b and a>c:\n",
    "        print('postive')\n",
    "    elif b>a and b>c:\n",
    "        print('negative')\n",
    "    else:\n",
    "        print(\"neutral\")\n",
    "\n",
    "score(pos, neg, neu)\n",
    "\n",
    "pos, neg, neu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added the sentiment in text form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_values(row):\n",
    "    if row['Positive'] > row['Negative'] and row['Positive'] > row['Neutral']:\n",
    "        return 'positive'\n",
    "    elif row['Negative'] > row['Positive'] and row['Negative'] > row['Neutral']:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment\"] = data.apply(compare_values, axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the sentiment trends into seperate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('Date')\n",
    "\n",
    "def most_common_sentiment(series):\n",
    "    return Counter(series).most_common(1)[0][0]\n",
    "\n",
    "# Create a new DataFrame with the required columns\n",
    "sentiment_logs = grouped.agg(\n",
    "    start_sentiment=('sentiment', 'first'),\n",
    "    end_sentiment=('sentiment', 'last'),\n",
    "    most_common_sentiment=('sentiment', most_common_sentiment)\n",
    ").reset_index()\n",
    "\n",
    "# Print the result DataFrame\n",
    "print(sentiment_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted sentiment at start and end of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('Date')\n",
    "\n",
    "# Add columns for the first and last Sentiment values within each group\n",
    "data['Start Conversation'] = grouped['sentiment'].transform('first')\n",
    "data['Stop Conversation'] = grouped['sentiment'].transform('last')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_words = [word for sublist in data['Lemmatized'] for word in sublist]\n",
    "\n",
    "fdist = nltk.FreqDist(combined_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most common 10 words are: \\n\")\n",
    "\n",
    "for i,j in fdist.most_common(10):\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tokenized_mgs\"] = data[\"Tokenized_words\"].apply(lambda x: \" \".join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['Tokenized_mgs'])\n",
    "\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_model.fit(X)\n",
    "\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print_topics(lda_model, vectorizer)\n",
    "\n",
    "\n",
    "topic_distribution = lda_model.transform(X)\n",
    "\n",
    "\n",
    "def most_relevant(topic_distribution):\n",
    "    most_relevant_topics = []\n",
    "    for distribution in topic_distribution:\n",
    "        most_relevant_topic = distribution.argmax()\n",
    "        most_relevant_topics.append(most_relevant_topic)\n",
    "\n",
    "    return most_relevant_topics\n",
    "\n",
    "relevant_topics = most_relevant(topic_distribution)\n",
    "\n",
    "\n",
    "data['LDA'] = relevant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer()\n",
    "X = tfvectorizer.fit_transform(data[\"Tokenized_mgs\"])\n",
    "\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=0)\n",
    "lsa_model.fit(X)\n",
    "\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for idx, component in enumerate(model.components_):\n",
    "        terms_in_topic = [terms[i] for i in component.argsort()[:-top_n - 1:-1]]\n",
    "        print(f\"Topic {idx}: {' '.join(terms_in_topic)}\")\n",
    "\n",
    "print_topics(lsa_model, tfvectorizer)\n",
    "\n",
    "\n",
    "topic_distribution = lsa_model.transform(X)\n",
    "\n",
    "\n",
    "def most_relevant(topic_distribution):\n",
    "    most_relevant_topics = []\n",
    "    for distribution in topic_distribution:\n",
    "        # Get the topic with the highest value\n",
    "        most_relevant_topic = distribution.argmax()\n",
    "        most_relevant_topics.append(most_relevant_topic)\n",
    "    return most_relevant_topics\n",
    "\n",
    "\n",
    "relevant_topics = most_relevant(topic_distribution)\n",
    "\n",
    "\n",
    "data['LSA'] = relevant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data[\"Tokenized_mgs\"])\n",
    "\n",
    "\n",
    "nmf_model = NMF(n_components=5, random_state=0)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_\n",
    "\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print_topics(nmf_model, vectorizer)\n",
    "\n",
    "\n",
    "def most_relevant(W):\n",
    "    most_relevant_topics = []\n",
    "    for distribution in W:\n",
    "        most_relevant_topic = distribution.argmax()\n",
    "        most_relevant_topics.append(most_relevant_topic)\n",
    "    return most_relevant_topics\n",
    "\n",
    "\n",
    "relevant_topics = most_relevant(W)\n",
    "\n",
    "\n",
    "data['NMF'] = relevant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Latent Semantic Analysis (pLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data[\"Tokenized_mgs\"])\n",
    "\n",
    "\n",
    "plsa_model = LatentDirichletAllocation(n_components=5, doc_topic_prior=0.1, topic_word_prior=0.1, random_state=0)\n",
    "plsa_model.fit(X)\n",
    "\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print_topics(plsa_model, vectorizer)\n",
    "\n",
    "\n",
    "def most_relevant(topic_distribution):\n",
    "    most_relevant_topics = []\n",
    "    for distribution in topic_distribution:\n",
    "        # Get the topic with the highest probability\n",
    "        most_relevant_topic = distribution.argmax()\n",
    "        most_relevant_topics.append(most_relevant_topic)\n",
    "    return most_relevant_topics\n",
    "\n",
    "\n",
    "topic_distribution = plsa_model.transform(X)\n",
    "\n",
    "\n",
    "relevant_topics = most_relevant(topic_distribution)\n",
    "\n",
    "\n",
    "data['pLSA'] = relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all the topic modelings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for idx, component in enumerate(model.components_):\n",
    "        terms_in_topic = [terms[i] for i in component.argsort()[:-top_n - 1:-1]]\n",
    "        print(f\"Topic {idx}: {' '.join(terms_in_topic)}\")\n",
    "\n",
    "\n",
    "def coherence_score(model, vectorizer, documents, top_n=10):\n",
    "    topics = model.components_\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    co_occurrences = defaultdict(int)\n",
    "    for topic in topics:\n",
    "        top_words = [words[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word_pair in combinations(top_words, 2):\n",
    "            co_occurrences[word_pair] += sum(1 for doc in documents if word_pair[0] in doc and word_pair[1] in doc)\n",
    "    coherence = sum(co_occurrences.values()) / len(co_occurrences)\n",
    "    return coherence\n",
    "\n",
    "# Compute coherence scores\n",
    "lda_coherence = coherence_score(lda_model, tfvectorizer, data[\"Tokenized_mgs\"])\n",
    "nmf_coherence = coherence_score(nmf_model, vectorizer, data[\"Tokenized_mgs\"])\n",
    "plsa_coherence = coherence_score(plsa_model, tfvectorizer, data[\"Tokenized_mgs\"])\n",
    "lsa_coherence = coherence_score(lsa_model, vectorizer, data[\"Tokenized_mgs\"])\n",
    "\n",
    "print(\"\\nCoherence Scores:\")\n",
    "print(f\"LDA Coherence: {print_topics(lda_model, tfvectorizer)}\\n\")\n",
    "print(f\"NMF Coherence: {print_topics(nmf_model, vectorizer)}\\n\")\n",
    "print(f\"PLSA Coherence: {print_topics(plsa_model,tfvectorizer)}\\n\")\n",
    "print(f\"LSA Coherence: {print_topics(lsa_model, vectorizer)}\\n\")\n",
    "\n",
    "print(\"\\nCoherence Scores:\")\n",
    "print(f\"LDA Coherence: {lda_coherence}\")\n",
    "print(f\"NMF Coherence: {nmf_coherence}\")\n",
    "print(f\"PLSA Coherence: {plsa_coherence}\")\n",
    "print(f\"LSA Coherence: {lsa_coherence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_diversity(topics, top_n=10):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        unique_words.update(top_words)\n",
    "        total_words += len(top_words)\n",
    "    return len(unique_words) / total_words\n",
    "\n",
    "lda_diversity = topic_diversity(lda_model.components_)\n",
    "nmf_diversity = topic_diversity(nmf_model.components_)\n",
    "plsa_diversity = topic_diversity(plsa_model.components_)\n",
    "lsa_diversity = topic_diversity(lsa_model.components_)\n",
    "\n",
    "print(f\"LDA Diversity: {lda_diversity}\")\n",
    "print(f\"NMF Diversity: {nmf_diversity}\")\n",
    "print(f\"pLSA Diversity: {plsa_diversity}\")\n",
    "print(f\"LSA Diversity: {lsa_diversity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating word clouds from the frequencies of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(data['Tokenized_mgs'])\n",
    "\n",
    "# Create a Document-Term Matrix using CountVectorizer for LDA\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(data[\"Tokenized_mgs\"])\n",
    "\n",
    "# Create Word Clouds\n",
    "def create_word_cloud(model, vectorizer, num_topics):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for idx, component in enumerate(model.components_):\n",
    "        word_freq = {terms[i]: component[i] for i in component.argsort()[:-11:-1]}\n",
    "        wordcloud = WordCloud(width=400, height=200, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Topic {idx}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nWord Clouds for LDA:\")\n",
    "create_word_cloud(lda_model, count_vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nWord Clouds for NMF:\")\n",
    "create_word_cloud(nmf_model, vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nWord Clouds for pLSA:\")\n",
    "create_word_cloud(plsa_model, count_vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nWord Clouds for LSA:\")\n",
    "create_word_cloud(lsa_model, vectorizer, num_topics)\n",
    "\n",
    "# Create Bar Charts\n",
    "def create_bar_chart(model, vectorizer, num_topics, top_n=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for idx, component in enumerate(model.components_):\n",
    "        top_terms = [(terms[i], component[i]) for i in component.argsort()[:-top_n - 1:-1]]\n",
    "        df_top_terms = pd.DataFrame(top_terms, columns=['Term', 'Weight'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x='Weight', y='Term', data=df_top_terms)\n",
    "        plt.title(f\"Top Terms for Topic {idx}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nBar Charts for LDA:\")\n",
    "create_bar_chart(lda_model, count_vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nBar Charts for NMF:\")\n",
    "create_bar_chart(nmf_model, vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nBar Charts for pLSA:\")\n",
    "create_bar_chart(plsa_model, count_vectorizer, num_topics)\n",
    "\n",
    "print(\"\\nBar Charts for LSA:\")\n",
    "create_bar_chart(lsa_model, vectorizer, num_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sentiments over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment\n",
    "def get_sentiment(text):\n",
    "    sentiment = sid.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "data['sentiment_compound'] = data['Tokenized_mgs'].apply(get_sentiment)\n",
    "\n",
    "# Aggregate sentiment scores by date\n",
    "data[\"Date\"] = pd.to_datetime(data['Date'])\n",
    "data['Date'] = data['Date'].dt.date\n",
    "daily_sentiment = data.groupby('Date')['sentiment_compound'].mean().reset_index()\n",
    "\n",
    "# Plot sentiment trends\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=daily_sentiment, x='Date', y='sentiment_compound', marker='o')\n",
    "plt.title('Sentiment Trend')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = sid.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "sentiments = data['Tokenized_mgs'].apply(get_sentiment)\n",
    "df = pd.concat([data, sentiments.apply(pd.Series)], axis=1)\n",
    "\n",
    "# Aggregate sentiment scores by date\n",
    "data[\"Date\"] = pd.to_datetime(data['Date'])\n",
    "data['Date'] = data['Date'].dt.date\n",
    "daily_sentiment = df.groupby('Date')[['pos', 'neu', 'neg', 'compound']].mean().reset_index()\n",
    "\n",
    "# Plot sentiment trends\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(daily_sentiment['Date'], daily_sentiment['pos'], marker='o', label='Positive', color='green')\n",
    "plt.plot(daily_sentiment['Date'], daily_sentiment['neu'], marker='o', label='Neutral', color='blue')\n",
    "plt.plot(daily_sentiment['Date'], daily_sentiment['neg'], marker='o', label='Negative', color='red')\n",
    "plt.plot(daily_sentiment['Date'], daily_sentiment['compound'], marker='o', label='Compound', color='purple')\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_counts = data.groupby('Date').size().reset_index(name='Message Count')\n",
    "\n",
    "# Plot message volumes over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=message_counts, x='Date', y='Message Count', marker='o')\n",
    "plt.title('Message Volumes Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Messages')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
